{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from mltu.tensorflow.model_utils import residual_block\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.preprocessors import ImageReader\n",
    "from configs import ModelConfigs\n",
    "from mltu.tensorflow.callbacks import TrainLogger\n",
    "from mltu.tensorflow.metrics import CWERMetric\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
    "    inputs = layers.Input(shape=input_dim, name='input')\n",
    "    input = layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    x1 = residual_block(input, 16, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "    x2 = residual_block(x1, 16, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x3 = residual_block(x2, 16, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x4 = residual_block(x3, 32, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x5 = residual_block(x4, 32, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x6 = residual_block(x5, 64, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "    x7 = residual_block(x6, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    squeezed = layers.Reshape((x7.shape[-3] * x7.shape[-2], x7.shape[-1]))(x7)\n",
    "    bi_LSTM = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(squeezed)\n",
    "    output = layers.Dense(output_dim + 1, activation=\"softmax\", name=\"output\")(bi_LSTM)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ModelConfigs()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = \"./dataset/IDCAR2015_Dataset/train/training_images/\"\n",
    "train_annotations_path = \"./dataset/IDCAR2015_Dataset/train/training_localization_transcription/\"\n",
    "\n",
    "val_images_path = \"./dataset/IDCAR2015_Dataset/val/test_images/\"\n",
    "val_annotations_path = \"./dataset/IDCAR2015_Dataset/val/test_localization_transcription/\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_file(image_folder, annotations_folder):\n",
    "    dataset, vocab, max_len = [], set(), 0\n",
    "\n",
    "    # Get sorted lists of image and annotation files to ensure they match\n",
    "    image_files = sorted(os.listdir(image_folder))\n",
    "    annotation_files = sorted(os.listdir(annotations_folder))\n",
    "\n",
    "    # Use zip to combine image and annotation files\n",
    "    for image_file, annotation_file in tqdm(zip(image_files, annotation_files)):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        annotation_path = os.path.join(annotations_folder, annotation_file)\n",
    "\n",
    "        # Read the annotation file\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split()\n",
    "                \n",
    "                # Assuming the annotation contains the image name and the label\n",
    "                label = line[1] if len(line) > 1 else \"UNKNOWN\"  # Handle missing label\n",
    "\n",
    "                # Append image path and label to the dataset\n",
    "                dataset.append([image_path, label])\n",
    "                vocab.update(list(label))\n",
    "                max_len = max(max_len, len(label))\n",
    "\n",
    "    return dataset, sorted(vocab), max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 3129.88it/s]\n",
      "500it [00:00, 3148.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_vocab, max_train_len = read_annotation_file(train_images_path, train_annotations_path)\n",
    "val_dataset, val_vocab, max_val_len = read_annotation_file(val_images_path, val_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_provider = DataProvider(\n",
    "    dataset=train_dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab))\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_data_provider = DataProvider(\n",
    "    dataset=val_dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab))\n",
    "        ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btjan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_dim=(configs.height, configs.width, 3), output_dim=len(configs.vocab))\n",
    "padding_token = len(configs.vocab)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate), loss=CTCloss(), run_eagerly=False)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(configs.model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=10, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "trainLogger = TrainLogger(configs.model_path)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
<<<<<<< HEAD
=======
   "outputs": [],
   "source": [
    "def plot_histogram(data, title, xlabel, ylabel):\n",
    "    plt.hist(data, bins=20, edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for correlation analysis\n",
    "def plot_correlation_matrix(num_bboxes, text_lengths, box_areas):\n",
    "    data = np.array([num_bboxes, text_lengths, box_areas])\n",
    "    corr_matrix = np.corrcoef(data)\n",
    "   \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n",
    "                xticklabels=['Num Bboxes', 'Text Lengths', 'Box Areas'],\n",
    "                yticklabels=['Num Bboxes', 'Text Lengths', 'Box Areas'])\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Boxes Statistics:\n",
      "Mean: 10.46, Median: 8.00, Std Dev: 9.14, Min: 1, Max: 90\n",
      "Text Length Statistics:\n",
      "Mean: 3.93, Median: 3.00, Std Dev: 1.77, Min: 2, Max: 21\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_bboxes, text_lengths = calculate_statistics(dataset)\n",
    "\n",
    "# Calculate standard statistics for bounding boxes\n",
    "bbox_mean = np.mean(num_bboxes)\n",
    "bbox_median = np.median(num_bboxes)\n",
    "bbox_std = np.std(num_bboxes)\n",
    "bbox_min = np.min(num_bboxes)\n",
    "bbox_max = np.max(num_bboxes)\n",
    "\n",
    "# Calculate standard statistics for text lengths\n",
    "text_len_mean = np.mean(text_lengths)\n",
    "text_len_median = np.median(text_lengths)\n",
    "text_len_std = np.std(text_lengths)\n",
    "text_len_min = np.min(text_lengths)\n",
    "text_len_max = np.max(text_lengths)\n",
    "\n",
    "# Optional statistics\n",
    "print(\"Bounding Boxes Statistics:\")\n",
    "print(f\"Mean: {bbox_mean:.2f}, Median: {bbox_median:.2f}, Std Dev: {bbox_std:.2f}, Min: {bbox_min}, Max: {bbox_max}\")\n",
    "print(\"Text Length Statistics:\")\n",
    "print(f\"Mean: {text_len_mean:.2f}, Median: {text_len_median:.2f}, Std Dev: {text_len_std:.2f}, Min: {text_len_min}, Max: {text_len_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot statistics\n",
    "plot_histogram(num_bboxes, 'Number of Bounding Boxes per Image', 'Number of Bounding Boxes', 'Frequency')\n",
    "plot_histogram(text_lengths, 'Text Length Distribution', 'Text Length', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 15s/step - loss: 910.6019 - val_loss: 872.1805\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 13s/step - loss: 773.9846 - val_loss: 786.9468\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 13s/step - loss: 635.3892 - val_loss: 691.2572\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 13s/step - loss: 496.0209 - val_loss: 534.4860\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 13s/step - loss: 367.4553 - val_loss: 344.8382\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 13s/step - loss: 255.4106 - val_loss: 198.5972\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 13s/step - loss: 172.3088 - val_loss: 120.5715\n",
      "Epoch 8/100\n"
=======
      "Total number of images: 500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_bboxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal bounding boxes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(num_bboxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage number of bounding boxes per image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(num_bboxes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage text length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(text_lengths)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_bboxes' is not defined"
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data_provider, \n",
    "    validation_data=val_data_provider, \n",
    "    epochs=configs.train_epochs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format and display the detected text\n",
    "def display_detected_text(dataset):\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"{'Detected Text from Dataset':^50}\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "    for index in range(len(dataset)):\n",
    "        _, _, texts = dataset[index]\n",
    "        image_name = dataset.image_filenames[index]\n",
    "        \n",
    "        print(f\"Image: {image_name}\")\n",
    "        print(f\"{'-' * 50}\")\n",
    "        \n",
    "        if texts:\n",
    "            for i, text in enumerate(texts, start=1):\n",
    "                print(f\"Text {i}: {text}\")\n",
    "        else:\n",
    "            print(\"No text detected.\")\n",
    "        \n",
    "        print(f\"{'=' * 50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all detected text\n",
    "# display_detected_text(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./dataset/test/test_images/img_1.jpg')\n",
    "orig = image.copy()\n",
    "(H, W) = image.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_W, new_H) = (320, 320)\n",
    "right_W = W / float(new_W)\n",
    "right_H = H / float(new_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, (new_W, new_H))\n\u001b[1;32m----> 2\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrozen_east_text_detection.pb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "image = cv2.resize(image, (new_W, new_H))\n",
    "net = cv2.dnn.readNet('frozen_east_text_detection.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.12.4"
>>>>>>> 9589be4be3301508b621b71d5f09ac244d78bf8b
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
