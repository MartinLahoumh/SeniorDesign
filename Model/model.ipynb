{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to Load images & transcriptions\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, images_dir, transcriptions_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transcriptions_dir = transcriptions_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def parse_transcrpition(self, transcription_file):\n",
    "        bounding_boxes = []\n",
    "        texts = []\n",
    "\n",
    "        with open(transcription_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(',')\n",
    "                coords = list(map(int, parts[:8]))\n",
    "                text = parts[8]\n",
    "                bounding_boxes.append(coords)\n",
    "                texts.append(text)\n",
    "            \n",
    "        return bounding_boxes, texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_filenames[index]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "        transcription_file_name = f\"gt_{image_name.replace('.jpg', '.txt')}\"\n",
    "        transcription_path = os.path.join(self.transcriptions_dir, transcription_file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        bounding_boxes, texts = self.parse_transcrpition(transcription_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, bounding_boxes, texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = TextDataset(images_dir=\"./dataset/test/test_images/\", transcriptions_dir=\"./dataset/test/test_localization_transcription/\")\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bounding_boxes(image_path, bounding_boxes, texts):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    for box, text in zip(bounding_boxes, texts):\n",
    "        points = np.array([[box[i], box[i + 1]] for i in range(0, 8, 2)], np.int32)\n",
    "        points = points.reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [points], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        cv2.putText(image, text, (box[0], box[1] - 10), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    cv2.imshow(\"Image with bounding Boxes\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./dataset/test/test_images/img_10.jpg\"\n",
    "bounding_boxes, texts = dataset.parse_transcrpition('./dataset/test/test_localization_transcription/gt_img_10.txt')\n",
    "visualize_bounding_boxes( image_path, bounding_boxes, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyticals of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(dataset):\n",
    "    num_bboxes = []\n",
    "    text_lengths = []\n",
    " \n",
    "    for _, bounding_boxes, texts in dataset:\n",
    "        num_bboxes.append(len(bounding_boxes))\n",
    "        text_lengths.extend([len(text) for text in texts])\n",
    " \n",
    "    return num_bboxes, text_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, title, xlabel, ylabel):\n",
    "    plt.hist(data, bins=20, edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for correlation analysis\n",
    "def plot_correlation_matrix(num_bboxes, text_lengths, box_areas):\n",
    "    data = np.array([num_bboxes, text_lengths, box_areas])\n",
    "    corr_matrix = np.corrcoef(data)\n",
    "   \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n",
    "                xticklabels=['Num Bboxes', 'Text Lengths', 'Box Areas'],\n",
    "                yticklabels=['Num Bboxes', 'Text Lengths', 'Box Areas'])\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Boxes Statistics:\n",
      "Mean: 10.46, Median: 8.00, Std Dev: 9.14, Min: 1, Max: 90\n",
      "Text Length Statistics:\n",
      "Mean: 3.93, Median: 3.00, Std Dev: 1.77, Min: 2, Max: 21\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_bboxes, text_lengths = calculate_statistics(dataset)\n",
    "\n",
    "# Calculate standard statistics for bounding boxes\n",
    "bbox_mean = np.mean(num_bboxes)\n",
    "bbox_median = np.median(num_bboxes)\n",
    "bbox_std = np.std(num_bboxes)\n",
    "bbox_min = np.min(num_bboxes)\n",
    "bbox_max = np.max(num_bboxes)\n",
    "\n",
    "# Calculate standard statistics for text lengths\n",
    "text_len_mean = np.mean(text_lengths)\n",
    "text_len_median = np.median(text_lengths)\n",
    "text_len_std = np.std(text_lengths)\n",
    "text_len_min = np.min(text_lengths)\n",
    "text_len_max = np.max(text_lengths)\n",
    "\n",
    "# Optional statistics\n",
    "print(\"Bounding Boxes Statistics:\")\n",
    "print(f\"Mean: {bbox_mean:.2f}, Median: {bbox_median:.2f}, Std Dev: {bbox_std:.2f}, Min: {bbox_min}, Max: {bbox_max}\")\n",
    "print(\"Text Length Statistics:\")\n",
    "print(f\"Mean: {text_len_mean:.2f}, Median: {text_len_median:.2f}, Std Dev: {text_len_std:.2f}, Min: {text_len_min}, Max: {text_len_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot statistics\n",
    "plot_histogram(num_bboxes, 'Number of Bounding Boxes per Image', 'Number of Bounding Boxes', 'Frequency')\n",
    "plot_histogram(text_lengths, 'Text Length Distribution', 'Text Length', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_bboxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal bounding boxes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(num_bboxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage number of bounding boxes per image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(num_bboxes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage text length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(text_lengths)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_bboxes' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of images: {len(dataset)}\")\n",
    "print(f\"Total bounding boxes: {sum(num_bboxes)}\")\n",
    "print(f\"Average number of bounding boxes per image: {np.mean(num_bboxes):.2f}\")\n",
    "print(f\"Average text length: {np.mean(text_lengths):.2f}\")\n",
    "print(f\"Median number of bounding boxes per image: {bbox_median:.2f}\")\n",
    "print(f\"Standard deviation of nbounding boxes per image: {bbox_std:.2f}\")\n",
    "print(f\"Minimum bounding boxes in an image: {bbox_min}\")\n",
    "print(f\"Maximum bounding boxes in an image: {bbox_max}\")\n",
    "print()\n",
    "print(f\"Median text length: {text_len_median:.2f}\")\n",
    "print(f\"Standard deviation of text length: {text_len_std:.2f}\")\n",
    "print(f\"Minimum text length: {text_len_min}\")\n",
    "print(f\"Maximum text length: {text_len_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format and display the detected text\n",
    "def display_detected_text(dataset):\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"{'Detected Text from Dataset':^50}\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "    for index in range(len(dataset)):\n",
    "        _, _, texts = dataset[index]\n",
    "        image_name = dataset.image_filenames[index]\n",
    "        \n",
    "        print(f\"Image: {image_name}\")\n",
    "        print(f\"{'-' * 50}\")\n",
    "        \n",
    "        if texts:\n",
    "            for i, text in enumerate(texts, start=1):\n",
    "                print(f\"Text {i}: {text}\")\n",
    "        else:\n",
    "            print(\"No text detected.\")\n",
    "        \n",
    "        print(f\"{'=' * 50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all detected text\n",
    "# display_detected_text(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./dataset/test/test_images/img_1.jpg')\n",
    "orig = image.copy()\n",
    "(H, W) = image.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_W, new_H) = (320, 320)\n",
    "right_W = W / float(new_W)\n",
    "right_H = H / float(new_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, (new_W, new_H))\n\u001b[1;32m----> 2\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrozen_east_text_detection.pb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "image = cv2.resize(image, (new_W, new_H))\n",
    "net = cv2.dnn.readNet('frozen_east_text_detection.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
