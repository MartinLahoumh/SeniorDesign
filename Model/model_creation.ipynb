{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from mltu.tensorflow.model_utils import residual_block\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.preprocessors import ImageReader\n",
    "from configs import ModelConfigs\n",
    "from mltu.tensorflow.callbacks import TrainLogger\n",
    "from mltu.tensorflow.metrics import CWERMetric\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
    "    inputs = layers.Input(shape=input_dim, name='input')\n",
    "    input = layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    x1 = residual_block(input, 16, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "    x2 = residual_block(x1, 16, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x3 = residual_block(x2, 16, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x4 = residual_block(x3, 32, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x5 = residual_block(x4, 32, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x6 = residual_block(x5, 64, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "    x7 = residual_block(x6, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    squeezed = layers.Reshape((x7.shape[-3] * x7.shape[-2], x7.shape[-1]))(x7)\n",
    "    bi_LSTM = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(squeezed)\n",
    "    output = layers.Dense(output_dim + 1, activation=\"softmax\", name=\"output\")(bi_LSTM)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ModelConfigs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = \"./dataset/IDCAR2015_Dataset/train/training_images/\"\n",
    "train_annotations_path = \"./dataset/IDCAR2015_Dataset/train/training_localization_transcription\"\n",
    "val_images_path = \"./dataset/IDCAR2015_Dataset/val/test_images/\"\n",
    "val_annotations_path = \"./dataset/IDCAR2015_Dataset/val/test_localization_transcription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_file(image_folder, annotations_folder):\n",
    "    dataset, vocab, max_len = [], set(), 0\n",
    "\n",
    "    # Get sorted lists of image and annotation files to ensure they match\n",
    "    image_files = sorted(os.listdir(image_folder))\n",
    "    annotation_files = sorted(os.listdir(annotations_folder))\n",
    "\n",
    "    # Use zip to combine image and annotation files\n",
    "    for image_file, annotation_file in tqdm(zip(image_files, annotation_files)):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        annotation_path = os.path.join(annotations_folder, annotation_file)\n",
    "\n",
    "        # Read the annotation file\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split()\n",
    "                \n",
    "                # Assuming the annotation contains the image name and the label\n",
    "                label = line[1] if len(line) > 1 else \"UNKNOWN\"  # Handle missing label\n",
    "\n",
    "                # Append image path and label to the dataset\n",
    "                dataset.append([image_path, label])\n",
    "                vocab.update(list(label))\n",
    "                max_len = max(max_len, len(label))\n",
    "\n",
    "    return dataset, sorted(vocab), max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:01, 676.08it/s]\n",
      "500it [00:00, 742.08it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_vocab, max_train_len = read_annotation_file(train_images_path, train_annotations_path)\n",
    "val_dataset, val_vocab, max_val_len = read_annotation_file(val_images_path, val_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data provider\n",
    "train_data_provider = DataProvider(\n",
    "    dataset=train_dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create validation data provider\n",
    "val_data_provider = DataProvider(\n",
    "    dataset=val_dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\btjan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btjan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_dim=(configs.height, configs.width, 3), output_dim=len(configs.vocab))\n",
    "padding_token = len(configs.vocab)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate), loss=CTCloss(), run_eagerly=False)\n",
    "#model.summary()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=10, verbose=1)\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "trainLogger = TrainLogger(configs.model_path)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\btjan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\legacy\\backend.py:666: The name tf.nn.ctc_loss is deprecated. Please use tf.compat.v1.nn.ctc_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btjan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
